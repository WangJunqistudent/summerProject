信息科学与技术学院2020级考研考公学生暑期留宿佐证材料.docx,D:/tool/Pycharm/summerProject/examples\信息科学与技术学院2020级考研考公学生暑期留宿佐证材料.docx,"息科学与技术学院2020级考研（考公）学生暑期留宿佐证材料

"
北航学报-基于GAN的高动态范围图像生成方法.pdf,D:/tool/Pycharm/summerProject/examples\北航学报-基于GAN的高动态范围图像生成方法.pdf,"北 京 航 空 航 天 大 学 学 报 
Journal of Beijing University of Aeronautics and Astronautics  
 
收稿日期 : 2020-09-14； 录用日期 : 2021-04-23; 网络出版时间 : 2021 -05-17 17:00  
网络出版地址 : kns.cnki.net/kcms/detail/11.2625. V.20210517.1523.003.html  
基金项目: 浙江省自然 科学基金（ LY20F010013）  
*通讯作者 : E-mail： DandanDing@hznu.edu.c n   
引用格式 ： 
 
 
  http://bhxb.buaa.edu.cn  jbuaa@buaa.edu.cn  
 DOI：10.13700/j.bh.1001 -5965. 2020.0518  
基于条件生成对抗网络的 HDR图像生成方法  
贝悦1，王琦1，程志鹏1，潘兴浩1，杨默涵2，丁丹丹2,*  
（1．咪咕视讯科技有限公司 ,  上海   201201；2．北京观止创想科技有限公司 ,  北京   100036)  
摘      要：高动态范围 (HDR)图像相比低动态范围 (LDR)图像有更宽的色域和更高的亮度范围，更
符合人眼视觉效果 ， 但由于目前的图像采集设备大都是 LDR设备，导致 HDR图像资源匮乏，解决该
问题的一种有效途径是通过逆色调映射将 LDR图像映射为 HDR图像。提出 了一种基于条件 生成对抗网
络(CGAN)的逆色调映射算法，以重建 HDR图像。为此，设计了基于多分支的生成 对抗网络与基于鉴别
块的鉴别网络，并利用 CGAN的数据生成能力和特征提取能力，将单张 LDR图像从 BT.709色域映射到
对应的 BT.2020色域。实验结果表明 ：与现有方法相比，所提出的网络能够获得更高的客观与主观质量 ，
特别是针对低色域中的模糊区域，所提方法能够重建出更清晰的纹理与细节。  
关  键  词：条件生成对抗网络 ; 卷积神经网络 ; 逆色调映射 ; 色域转换 ; 特征提取  
中图分类号： TP391                       
文献标识码： A                           
 
 
 
随着显示设备技术产业的发展，普通 低动态
范围 (Low  Dynamic Range ，LDR)的显示器逐渐不
能满足大众影音需求，具有更高亮度、更广色域
的高动态范围 (High  Dynamic Range ，HDR)显示设
备逐渐出现。与普通的 LDR图像相比， HDR图
像的编码位数由 8 bit提升至 10 bit或以上，图像
色域范围由 BT.709标准提升至 BT.2020标准， 能
提供更多的动态范围和图像细节 。然而，目前与
HDR显示设备相适配的视频图像资源十分匮乏。
因此，通过逆色调映射，将原本的 LDR图像生成
对应的高质量 HDR图像，也就是逆色调映射，
是有效提升原有图像视频质量的有效途径之一
[1]。 
从LDR图像到 HDR图像的映射是一个非线
性的、不适定问题，存在多种解。与 LDR图像相
比， HDR图像色域更广，广义上的 HDR图像色
域应能覆盖到 90%以上的 DCI-P3色域范围 ; 位
深由 LDR图像的 8 bit增加到 10 bit。因此，在不同场景下，由 LDR图像生成对应的 HDR图像需
要良好的对应点像素生成能力。由此启发可以使
用神经网络来完成上述逆映射。  
卷 积 神 经 网 络 （ Convolutional Neural 
Networks ，CNN）首先被用于上述逆色调映射，
以完成 HDR重建。 实验结果表明， 一些基于 CNN
的方法能够 获得比传统方法更高的性能[]-[7]。但
是，这些基于 CNN的方法通常涉及大量参数，
因而在训练中会花费很长时间。此外，在网络训
练中往往使用 L2损失函数，虽然获得了较高的
峰值信噪比 (Peak Signal -to-Noise Ratio ，PSNR)，
但图像主观质量偏于平滑，降低了 HDR的观看
体验。近 年来，为克服上述问题，研究人员提出
使用基于对抗生成网络（ Generative Adversarial 
Network，GAN）的 HDR重建方法。 以往工作[]-[11]
一般都使用 U-net做为 GAN的生成器， 使用卷积
配合降采样的结构作为鉴别器， 提高了重建 HDR
的主观性能。这些研究表明了 GAN网络在 HDR2                                                      北京航空航天大学学报   
[键入文字 ] 
 重建方面具有巨大 的潜力。 GAN网络无需要求一
个假设的数据分布，而是使用一种分布直接进行
采样，从而达到理论上可以完全逼近真实数据，
这也是 GAN网络最大的优势。但是， GAN网络
无需预先建模的方法对于较大图像而言，生成目
标的可控性不强， 无法生成具有特定性质的图像。  
受上述工作的启发， 本文提出一种基于条件
生成对抗网络 (Conditional Generative Adversarial 
Network，CGAN)的从单张 LDR图像生成对应
HDR图像的方法。利用 CGAN网络的思想， 通过
在GAN网络的生成网络和鉴别网络中添加条件
性的约束，来解决训练过于自由的问题，实 现了
由单张 LDR图像到对应 HDR图像的端到端的非
线性映射。实验结果表明， 本文方法不仅能够将 8 
bit的LDR图像自然地映射到 10 bit的HDR图像， 还
能挖掘 LDR图像中的一些隐藏特征，与以往方法
相比，所提方法获得了较高的主观与客观质量。  
1  相关工作  
基于神经网络方法以端到端的方式解决逆色
调映射问题具有巨大潜力。  
1.1 多曝光的方法  
多曝光的方法是处理逆色调映射的最直接方
法，其合并不同曝光拍摄的多个 LDR图像，最终
生成单个 HDR图像。不过，一般多曝光的图像
获取需要使用专业相机，这对于实际应用并 不友
好，因为实际应用中所涉及的图像几乎都是非专
业用户拍摄的单曝光图像。为此，普遍采用的方
式是：首先，使用软件算法从单个输入 LDR图像
生成若干个不同曝光的 LDR图像；然后，使用这
些生成的 LDR图像重建出对应的 HDR图像。如
此，多曝光方法一般包括 2个步骤，即生成和重
建，且每个步骤都可以由神经网络来实现。  
基于上述多曝光的思想， Endo等[2]开发了一
对基于“编码器 -解码器”结构的 CNN网络
DrTMO，包括向上曝光模型和向下曝光模型，分
别推断 2组曝光的 LDR图像， 使用已有的合并方
法将这些 LDR图像合并为一个 HDR图像。 Lee
等[7]提出了一个 CNN链式结构， 采用 2个并行的
CNN，每个 CNN推断 3个LDR图像。后来 ，Lee
等[10]基于 GAN的结构进一步改进了设计，使用
2个神经网络 G_plus和G_minus 来生成曝光图
像。Xu等[8]将GAN用于 LDR图像的合并，先利
用相机响应曲线来更改输入 LDR图像的曝光时间， 并获得 2个曝光过度和 2个曝光不足的图像，
训练了一个 GAN网络来合并这些 LDR图像以进
行HDR图像重建。  
1.2 单曝光的方法  
相对于多曝光 的方法，单曝光 的方法将输入
的LDR图像直接转换为输出的 HDR图像，灵活性
更高。  
1.2.1 基于 CNN的单曝光  
Takeuchi等[12]提出了一种基于 CNN的、从
BT.709到BT.2020的色彩空间转换方法。 Eilertsen
等[3]基于自动编码器网络重构输入 LDR图像中的
过饱和区域，所提出的 HDRCNN 方法在图像中的
过饱和像素比例低于 5％时，可以达到较好 的效
果。 Marnerides 等[5]设计了一个 ExpandNet，LDR
图像通过局部 、中等和全局 3种CNN分支后进行
融合，得到对应的 HDR图像。 ExpandNet 能够提
升一些图片的主观质量，但在某些图像上，会出
现颜色偏移或者主观质量不高等问题。 Kinoshita
和Kiya[6]在现有 U-net的基础上，添加了一个全局
编码器，以和现有的 U-net编码器并行工作，来自
2个编码器的结果被一起并送入 U-net解码器进行
HDR重建。  
1.2.2 基于 GAN的单曝光  
GAN网络已经在很多方面取得了巨大的进
展，如图像超分辨率[]-[14]、图像风格迁移[]-[16]、
人体姿态估计[17]、人体运动传递[18]等。在逆色调
映射方面， GAN网络刚刚起步。 Ning等[9]基于
U-Net设计了生成器，并采用了基于 CNN的鉴别
器。 Lee等[10]基于 GAN进行超分辨率与 HDR的联
合重建。 以上方法都是基于现成的网络展开，针
对逆色调映射，有必要设计新的网络结构配合新
的损失函数，以获得更好的重建效果。  
1.3 GAN网络和 CGAN网络  
GAN网络由 Goodfellow 等[19]提出，是一种基
于对抗生成的博弈思想创造出的模型， 其组成有 2
个网络，分别为生成网络 G和鉴别网络 D。生成网
络和鉴别网络相互对抗，所采用的 思想是零和博
弈思想，即双方在平等的情况下开局，各自使用
各自的策略去与对方对抗，在对抗中双方再利用
对方的策略及时对自己的策略进行调整。双方充
分遵循“损人利己”去实现获得胜利的目的。这个
思想拓展到深度学习领域，即生成网络和鉴别网
络为博弈的双方，生成网络不断发挥自己拟合数                                             贝悦等： 基于条件生成对抗网络的 HDR图像生成方法                                 
3 
 
 据的能力去欺骗鉴别网络，而鉴别网络则尽力去
辨别生成网络传 送来的拟合数据，两者的最终目
标是达到纳什平衡[20]，最终让生成网络学习到真
实的数据分布。  
如图 1所示，生成网络 G根据给出的标签样本
的数据分布，将服从随机分布的噪声
z 包装成真
实的样本数据， 该过程也被称为生成样本数据的
过程。生成网络用 该样本数据去欺骗鉴别网络。
鉴别网络 D则用来鉴别生成网络输出的样本数据
的真假性。  
 
图1  GAN网络基本结构  
Fig.1  Basic structure of GAN  network   
生成器为了能够学习真实的样本分布
data()P x
，先通过随机分布的噪声
z （一般设置为
高斯分布）创建映射
( ; )g Gz ，再通过鉴别器的映
射
( ; )d Dx 判断所生成的数据是否符合真实样本
的分布，其中 G、D是由参数为 θg、θd的神经网
络表示的微分函数 。在 GAN网络中，通常采用
的代价函数为  
data ) ()D
(min max ( , )
([ (log  log 1 [ )] ( ( ( )))])PPGV D G
E D E D G +−=
z x x z z xz
（1） 
式中：
x为真实数据分布 ；最大化
()Dx 表示鉴
别器将尽可能地学习真实样本数据分布 ；
()Pzz
为输入的噪声分布 ；
()Gz表示生成器生成的样本
数据分布 ；
( ( ))DG z 表示鉴别器将尽力去鉴别生成
器生成的样本数据是否为真。在训练过程中，先
将两者之一参数固定，优化另一方，一段时间后
交换固定和优化角色。如此反复博弈，生成器和
鉴别器最终将达到纳什平衡。  
理论上， GAN网络有能够拟合任何复杂样本
数据的可能性，但在实际应用中， GAN在拟合复
杂样本数据时表现不够稳定。因此 ，GAN网络被
提出。在 GAN网络基础上， CGAN网络通过加
入条件的方式加强对数据生成的引导，加入的条
件是任意的，如标签信息、数据属性信息等。在
CGAN网络中，生成网络接收的就是随机噪声
z
和加入的条件
c 。因此， CGAN的代价函数一般为  
data() )D
(m
)(in max ( ,
  log |  l([ ( )] [ ( ( )))] og 1 |)
 =
PG
P E D cG
E G cVD
D +−
z x x z z xz
（2） 
通过这种方式，原本的 GAN网络从无监督
或者半监督的学习转化成有监督的学习。 CGAN
网络和 GAN网络一样都是相互交替训练。 CGAN
网络从训练鉴别网络开始，再训练生成网络。可
见， CGAN网络可以从添加的条件中得到需要的
生成信息，从而有利于图像生成任务的进行。  
2  所提出的 逆色调映射算法  
2.1 生成网络  
受Marnerides 等[5]采用多分支网络分别提取
低频、中频和高频特征等工 作的启发，本文所设
计的生成网络的结构如图 2所示。该模型使用
256 256
大小的 LDR图像作为输入， 通过 3个分
支后，对得到 的3个特征图进行融合并输出。在
layer层中，卷积核尺寸为
33 ，边界使用 1作为
填充，并使用 RELU激活函数。由于使用目的不
同，步长在不同的分支中略有不同。  
1) 分支一。使用了 6个layer层和一个
44 的
卷积层， layer层使用的卷积核大小为
33 ，步长
为2，这是为了将输入的
256 256 大小的图片汇
聚为 1个像素点，以获得整张图片的感受野，从
而获得色调映射前图像的整体色彩变化，本分支
最终输出
1 1 64 尺寸的特征图。  
2) 分支二。使用 3个layer层和一个
33 的
卷积层， layer层步长是 1，以利于生成网络学习
到LDR图像的中频信息的分布。在该网络分支
中，输入图像的通道数是 3，输出图像的通道数
是64。 
3) 分支三。仅由 2个layer层组成， 每 个layer
层都使用
33 的卷积核，步长为 1。这使得该分
支能够获得
55 的感受野，从而能够获得图像局
部较为剧烈的变化特征，该分支最终得到
256 256 128
尺寸的特征图。  
所输入的 LDR图像经过 3个分支后， 先将分
支一获得的能够反映全局色调变化的一个像素点
扩充为
256 256 大小，这样是为了在下面的融合
中，使图像能够反映映射前全局色调变化，并与
另外 2个分支生成的特征图按照维度串联成
256 256 256
大小，最终通过一个 layer层将特
征图数目减少到 3，得到
256 256 3 尺寸的生成4                                                      北京航空航天大学学报   
[键入文字 ] 
 图像，该layer层中使用的激活函数是 SELU。 
 
图2  所提出的生成网络结构  
Fig.2  Structure of the proposed generative network  
2.2 鉴别网络  
鉴别网络用于判别生成图像是否符合真实的
数据分布。在本 文方法中，鉴别网络的结构如图
3所示。  
      
  
图3  所提出的鉴别网络及其内部鉴别块  
Fig.3  Structure  of the proposed authentication network and 
its internal authentication block  
将生成网络生成的图片和原 HDR图像按照
通道数串联成
256 256 6 大小，使用图 3所示的
鉴别块（ block）进行下采样。其中，每个鉴别块
使用卷积核为
44 ，步长为 2，边界填充为 1的
卷积层，并跟随归一化处理，使用 0.2的
LeakyRELU 激活函数。 图像依次经过 4个鉴别块，
每经过一个鉴别块后特征图都会缩小 2倍，在得
到
32 32 512 尺寸的图片之后，使用一层与鉴别
块相同的卷积层。鉴别器将最终的结果进行是或
非的判别。  
2.3 代价函数  
鉴别网络对真实样本的分布通过鉴别后判断
为真（即输出 1），对生成网络生成的样本进行鉴别后输出为假（即输出 0）。生成网络则尽力
使自己生成的样本分布获得真的评价（即获得
1）。因此，网络的一部分 损失函数可以表示为式
（2）。一方面，鉴别网络要对真实的样本分布经
过鉴别网络后生成的分布与相同大 小的全 1矩阵
做交叉熵，用于识别真实的样本分布；另一方面，
生成网络生成的样本分布经过鉴别网络后，与相
同大小的全 0矩阵做交叉熵，用于使得鉴别器尽
可能地识别虚假的数据分布。为了使生成网络生
成的数据分布能够尽力欺骗鉴别网络，将生成网
络生成的数据分布与全 1矩阵做交叉熵后再送入
鉴别网络。此外，在生成网络的 损失函数中，使
用均方误差 （Mean  Square  Error，MSE）进行逐
像素的差别计算，以保证生成图像和真实样本图
像之间的相似性。其中， MSE的表达如下：  
2
11
MSE ( ) ( )
2n
ii
iy x a x
n==−
 （3） 
式中：
y 为真实的数据分布 ；
a为生成网络生成
的数据分布 ；
ix为对应的像素分布。  
最终，整个网络的 损失函数为  
DLoss min max ( , ) MSE
GV G D =+
 （4） 
3  实验结果与分析  
3.1 实验设置  
通过从海量视频中收集了 8262张HDR高清
图像，在逐一剔除不良数据之后，选取了 6736
张大图像， 再将这些大图像逐一剪裁成 30312张
尺寸为
960 480 的小图像。在剔除相似场景后，
最终得到 7632张
960 480 大小的图像。 本实验选
取其中 4000张图像，并随机剪裁成
256 256 大
小，用来作为标签数据。对于这些图像，使用
OpenCV中的色调映射 算法随机进行映射，得到
的图像被作为对应的 LDR图像样本。 除以上 4000
张训练图像外， 从获取 的7632张图像中去掉训练
集后，随机选取不重复的 20张图像作为测试集，
测试集的缩略图如图 4所示。                                               贝悦等： 基于条件生成对抗网络的 HDR图像生成方法                                 
5 
 
 
 
图4  实验所使用的 20张LDR测试图片  
Fig.4  20 LDR test pictures used in this experiment  
本实验使用 Adam优化器，生成网络的学习
率设为
-310 ，鉴别网络的学习率设 为
-45 10 。在
训练的过程中，即使已经将生成网络的学习率设
置成鉴别网络的 2倍，判别网络 也总能轻而易举
地在对抗中取得上风，这样就会导致生成网络的
损失函数几乎没有下降，产生 GAN网络常见的
梯度消失问题， 损失函数容易陷入局部最优解。
因此，训练中，在更新
k 次生成网络参数的同时
只更新一次鉴别网络的参数，以保证控制鉴别网
络的鉴别能力与生成网络的生成能力尽量持平。
如果
k值过大，就会让生成网络的 损失函数的值
来回震荡 ; k值过小，鉴别网络 会达到局部最优
点，产生梯度消失问题。在实验中，根据训练集
大小，让生成网络更新 5次，鉴别网络更新 1次。
如此，模型能够较快收敛且重建图像的质量较为
稳定。  
实验基于 Pytorch平台展开，使用 CPU 
i9-9900K CPU@3.60GHz 处理器， 32GB 运行内
存， NVIDIA GeForce RTX 2080Ti GPU 进行训练
与测试。网络训练时间约为 40h。 
3.2 实验结果  
使用所得到的模型对 图4所示的 20张图像进
行测试，并给出了客观与主观测试结果。  
使用 PSNR、MPSNR、SSIM、MS-SSIM及
HDR -VDP共5个客观指标对 HDR重建图像的质
量进行评价。其中， MS-SSIM客观反映了图像在
多尺度上的结构相似性， HDR -VDP用算法来模
拟人眼观看图像的过程。 MS-SSIM与HDR -VDP
是用来评价 HDR重建图像质量的重要指标。使
用不同的方法对图  4所示的测试集进行测试，所
得到的平均值如表 1所示。可以看出，与以往方法DrTMO[2]、ExpandNet[5]与HDRCNN[3]相比，
本文方法获得了最高 SSIM、MS-SSIM与
HDR -VDP值，反映了所提方法能够较好地重建
出HDR图像。  
此外，还对比了不同方法所得到的 HDR图
像的主观质量。由于普通显示器无法显示 HDR
图像，将所有方法得到的 HDR图像都通过同样
的方法再映射为 LDR图像， 以使它们能够在普通
显示器上可视化。 如图 5所示，DrTMO[2]方法生
成的图像偏白，与原 HDR图像差距较大；
ExpandNet[5]在有些图像上有严重的色度失真， 影
响了主观质量； HDRCNN[3]对于曝光区域低于 5%
的图像有较好效果，但是也也会产生色度偏移，
如第二行右上角的天空有明显的色度偏移。与上
述方法相比， 本文方法可以得到质量较高的 HDR
图像，图像看上去自然，与原 HDR图像较为接
近。ExpandNet[5]方法中出现的色彩偏移等现象也
得到了解决。 此外，所训练得到的模型不仅仅针
对某一类场景，在各种场景，各种亮度条件下都
能得到较好质量的重建图像。 如 图6所示， 在 LDR
图像的 BT.709色域难以显示的模糊特征也能够
在所得到的 HDR图像中有更加清晰的表达；如
图7所示，在明亮场景下重建 的图像色彩自然，
模糊区域也有一定程度的亮度提升，并且没有明
显的色彩偏移现象。  
4  消融实验  
为了验证多分支网络的有效性，对生成网络
进行了消融研究。  
首先，去掉了提取中频信息的分支， 所得实
验结果如表 2所示。可以看出，去掉中频分支 后
的生成网络所获得的客观指标数值均低于所提出
的三分支网络 ，获得的主观图像质量的色彩饱和
度不足，亮度不够高，如 图8（a）所示。 
然后，为进一步验证多分支网络的有效性，
分别对 3个分支进行了单独训练，实验 结果如表
2所示。图 8展示了保留不同分支后的主观结果 ，
可以看出 ，单独分支 所生成的图像 存在明显的 偏
色，且纹理细节 不够丰富 。值得注意的是， 由于
低频分支 会将输入的
256 256 大小的图像汇聚到
1个像素点，以获得整张图片的感受野， 低频分
支最终的输出尺寸是
11 。为直观 地展示低频分
支的输出图像， 将其放大为
256 256 大小的图像 ，
如图 8（d）所示。 4种消融实验证明， 本文所提6                                                      北京航空航天大学学报   
[键入文字 ] 
 出的生成网络 能够显著 提高图像的质量，使得生
成图像色彩饱和度更高，主观质量更好，更加接
近参考图像。  
此外，将 3个分支输出的特征图进行了可视
化，结果如图 9所示。  
5  结 论 
基于 GAN网络架构，本文提出了一种 LDR
图像到 HDR图像的逆色调映射方法。 （1）为此，
我们设计了一种新的生成网络， 使用多分支结构，提取图像 低频、中频、 高频特征，并将不同尺度
的特征进行融合得到最终的 HDR图像；（2）设
计了一种新的鉴别器网络，使用组合的鉴别块完
成鉴别任务。 （3）实验结果表明，所提出的方法
在客观指标上超过了以往的方法，尤其是在直接
反映 HDR重建图像质量的 MS-SSIM与
HDR -VDP指标上取得了较高性能。在主观质量
上，所重建的 HDR图像主观质量自然并且无显
著色彩失真。  
 
LDR图像 HDR图像 DrTMO ExpandNet HDRCNN 所提出的方法
图 5  不同方法得到的 HDR图像的主观效果对比  
Fig.5  Comparison of subjective effects of HDR images obtained by different methods  
表 1 不同方法的客观性能比较  
Table 1 Comparison of objective performance among  different methods  
方法  PSNR  MPSNR  SSIM  MS-SSIM  HDR -VDP -2 
DrTMO[2] 22.31  22.44  0.58 0.59 63.79  
ExpandNet[5]  23.61  23.79  0.70 0.71 78.02  
HDRCNN[3] 25.70  25.95  0.60 0.63 70.93  
所提出的方法  24.99  25.26  0.71 0.77 78.67  
                                              贝悦等： 基于条件生成对抗网络的 HDR图像生成方法                                 
7 
 
 表 2 消融实验：网络中不同分支 的客观性能比较  
Table 2 Comparison of objective performance of different branches of network  in ablation  experiment  
方法  PSNR  MPSNR  SSIM  MS-SSIM  HDR -VDP -2 
所提出的方法  24.99  25.26  0.71 0.77 78.67  
去掉中频分支  22.06  22.46 0.61 0.65 77.73  
仅中频分支  22.36  30.46  0.42 0.45 77.54  
仅高频分支  22.84  42.44  0.45 0.47 78.72  
               
  
图6  低色域模糊场景下的 HDR图像重建                   图7  一般场景下的 HDR图像重建  
(左： LDR，右：重建 HDR )                           (左： LDR，右：重建 HDR ) 
Fig.6  HDR image reconstruction in low -color -gamut blurred scene 。 Fig.7  HDR  image reconstruction in general scenes  
(Left: LDR, Right: Reconstructed HDR)                         (Left: LDR, Right: Reconstructed HDR)  
 
图8  消融实验：保留网络中不同分支 所得到的主观图像质量  
（左 1：去掉中频分支，左 2：仅中频分支，左 3：仅高频分支 ，左 4：仅低频分支，左 5：所提出方法 :） 
Fig.8  Subjective image quality by retaining different branches of network  in ablation  experiment  
（Left 1: Remove the middle -frequen cy branch , Left 2: Only middle-frequenc y branch , Left 3: Only high -frequenc y branch ,  
Left 4: Only low-frequenc y branch,  Left 5: Proposed method ） 
 
图9  不同分支输出的特征图（左： 低频分支 ，中：中频分支 ，右：高频分支 ） 
Fig.9  Feature map s output from  different branch es 
 (left: low-frequenc y branch, middle: middle -frequenc y branch , right: high-frequenc y branch ) 
 
  
 8                                                      北京航空航天大学学报   
 
 参考文献（ References ） 
 
[1] 马正先 . HDR技术及其在 4K超高清电视上的应用 [J]. 电视
技术 , 2019, 43(1):33 -39.               
MA Z X.HDR technology  and application  on 4K 
ultra-high-definition  TV[J].Television Technology ,2019, 
43(1):33 -39(in  Chinese).  
[2] ENDO  Y ,KANAMORI Y ,MITANI J.Deep reverse tone 
mapping[J ].ACM Transactions on 
Graphics, 2017,36(6):177:1 -177:10.   
[3] EILERTSEN G,KRONANDER J,DENES G,et al.HDR image 
reconstruction from a single exposure using deep 
CNNs[J ].ACM Transactions on Graphics,  2017,36(6):1 -15. 
[4] XU Y C,SONG  L,XIE R, et al. Deep video inverse tone 
mapping[C]//2019 IEE E Fifth International Conference on 
Multimedia Big Data (BigMM). Piscataway :IEEE Press, 
2019:142 -147. 
[5] MARNERIDES  D,BASHFORD ‐ROGERS  T,HA TCHETT  J, 
et al.ExpandNet:A  deep convolutional neural network for high 
dynamic range expansion from low dynamic range 
content[C]//Computer Graphics Forum ,2018,37(2):37 -49.  
[6] KINOSHITA Y ,KIY A H.iTM -Net: Deep inverse tone mapping 
using novel loss function considering tone mapping 
operator[J].IEEE Access,2019,7:73555 -73563.   
[7] LEE S,AN G H,KANG S J.Deep chain HDRI: Reconstructing 
a high dynamic range image from a single low dynamic range 
image[J ].IEEE Access,2018,6:49913 -49924.  
[8] XU Y C,NING S Y,XIE R,et al.Gan based multi -exposure 
inverse t one mappin g[C]//2019 IEEE International 
Conference on Image Processing (ICIP). Piscataway :IEEE 
Press,2019:1 -5.  
[9] NING S Y,XU H T,SONG L,et al.Learning an inverse tone 
mapping network with a generative adversarial 
regularizer[C]//2018 IEEE International Confe rence on 
Acoustics, Speech and Signal Processing (ICASSP). 
Piscataway :IEEE Press,2018:1383 -1387.   
[10] LEE S,AN G H,KANG S J.Deep recursive HDRI : Inverse 
tone mapping using generative adversarial 
networks[C]//Proceedings of the European Conference on 
Computer V ision (ECCV).  Berlin :Springer,2018:596 -611.  
[11] RONNEBERGER O,FISCHER P,BROX T.U -net: 
Convolutional networks for biomedical image 
segmentation[C]//International Conference on Medical image 
Compu ting and Computer -Assisted Inter vention.  
Berlin :Springer, 2015:23 4-241. 
[12] TAKEUCHI M,SAKAMOTO Y ,YOKOY AMA R,et al.A 
Gamut -extension method considering color information 
restoration using convolutional neural net works[C]//2019 IEEE International Conference on Image Processing 
(ICIP). Piscataway :IEEE Press,2019:774 -778. 
[13] LEDIG  C,THEIS L,HUSZÁR F,et al.Photo -realistic single 
image super -resolution using a generative adversarial 
network[C]//Proceedings of the IEEE Confe rence on 
Computer Visi on and Pattern Recog nition. Piscataway :IEEE 
Press,2017:4681 -4690.  
[14] W ANG X T,KE Y,WU S X,et al.EsrGAN:Enhanced 
super -resolution generative adversarial 
networks[C]//Proceedings of the European Conference on 
Computer Vision (ECCV). Berlin :Springer,2018:0 -0. 
[15] ISOLA P,ZHU J Y ,ZHOU T,et al.Image -to-image translation 
with conditional adversarial netwo rks[C]//Proceedings of the 
IEEE Confere nce on Computer Vi sion and Pattern 
Recogni tion.Piscataway :IEEE  Press ,2017:1125 -1134.  
[16] ZHU J Y ,PARK T,ISOLA P,et al.Unpaired image -to-image 
translation using cycle -consistent adversarial 
networks[C]//Proceedings of the IEEE International 
Conferenc e on Computer Visi on.Piscataway :IEEE 
Press,2017:2223 -2232.  
[17] SIAROHIN A,SANGINETO E,LA THUILIÈRE S,et 
al.Deformable GANs for pose -based human image 
generation[C]//Proceedings of the IEEE Conference on 
Computer Vision and Pattern Re cognition. Piscataway : IEEE 
Press,2018:3408 -3416.  
[18] CHAN C,GINOSAR S,ZHOU T H,et al.Everybody dance 
now[C]//Proceedings of the IEEE International Conference on 
Computer Vision. Piscataway :IEEE Press,2019:5933 -5942.  
[19] GOODFELLOW I,POUGET -ABADIE J,MIRZA M,et al. 
Generative adversarial nets[C]//Advances in Neural 
Information Processing System s，2014:2672 -2680.  
[20] RA TLIFF L J,BURDEN S A,SASTRY S S.Characterization 
and computation of local Nash  equilibria in continuous 
games[C]//2013 51st Annual Allerton Conference on 
Com munication,Control,and Computing. Piscataway :IEEE 
Press,2013:917 -924. 
 
 
 
 
 
 
 
 
 
 
 
HDR image generation method based on conditional  generative advers arial                                              贝悦等： 基于条件生成对抗网络的 HDR图像生成方法                                 
9 
 
 network  
BEI Y ue1, Wang Qi1, CHENG Zhipeng1, PANG Xinghao1, YANG  Mohan2, DING Dandan2* 
（1. MIGU Video Co., Ltd.,  Shanghai 201201,  China； 
2. Beijing Bravo Video Technologies Incorporation , Beijing 100 036, China） 
Abstract : Compared with Low Dynamic Range (LDR) images, High Dynamic Range (HDR) images have 
a wider color gamut and higher brightness range, which is more in line with human visual effects. However, 
since most of the current image acquisition devices are LDR device s, HDR image resources are scarce. An 
effective way to solve this problem is to map LDR images to HDR images through inverse tone mapping. This 
paper proposes an inverse tone mapping algorithm based on Conditional Generative Adversarial Network 
(CGAN) to r econstruct HDR images. To this end, a multi -branch -based generation network and a discrimination  
network based on discrimination  blocks are designed, and the data generation and feature extraction capabilities 
of CGAN are used to map a single LDR image fro m the BT.709 color gamut to the corresponding BT.2020 
color area. The e xperimental results show that the proposed network can obtain higher objective and subjective 
quality compared with the existing methods. Especially for fuzzy areas in the low color gam ut, the proposed 
method can reconstruct clearer textures and details.  
 
 Keywords : conditional  generative adversarial network; convolutional neural network; inverse tone 
mapping; gamut mapping; feature extraction  
 
 
 
———————————————  
Received: 2020 -09-14; Accepted: 2021-04-23; Published online: 2021 -05-17 
URL:  https://kns.cnki.net/kcms/detail/11.2625.v.20210517.1523.003.html  
Foundation item: Zhejiang Provincial Natural Science Foundation of China ( LY20F010013 ) 
*Corresponding author.  E-mail: DandanDing@hznu.edu.cn  
 "
水下图像英文论文-吕张凯-陈龙An Efficient Learning-based Framework for Underwater Image Enhancement.pdf,D:/tool/Pycharm/summerProject/examples\水下图像英文论文-吕张凯-陈龙An Efficient Learning-based Framework for Underwater Image Enhancement.pdf,"A Efﬁcient Learning-based Framework for
Underwater Image Enhancement
ZhangKai Lv, Long Chen, and Dandan Ding
School of Information Science and Engineering, Hangzhou Normal University
Hangzhou, China, 311121
Email: DandanDing@hznu.edu.cn
Abstract —Underwater image enhancement, which targets re-
moving the noise, scattering effect, as well as the bluish,
greenish, or yellowish tone from the underwater images, has
attracted much attention. Earlier rule-based methods generally
assume some prior knowledge when performing constrained
optimization. Recently, the learning-based methods have exhib-
ited superior performance over the rule-based methods. This
paper proposes a simple yet effective convolution neural network
(CNN)-based framework to enhance underwater images. Our
method consists of two stages, CNN-based enhancement and
YUV-based post-processing. In the ﬁrst stage, a lightweight CNN
network is developed to extract the latent features from the
input images for enhancement. Speciﬁcally, our CNN cascades
three residual groups which utilize channel attention blocks
to strengthen the feature extraction capability. In the second
stage, the output of the CNN model is transformed to the YUV
color space where the luminance component is further corrected,
improving the brightness of the whole image. Our model is
trained using the UIEB data set and tested on 100 underwater
images. Experimental results show that our method outperforms
state-of-the-art methods both qualitatively and quantitatively.
Moreover, in terms of computational complexity, our CNN model
costs 437k parameters, which is only 39.5% and 22.5% of the
existing CNN-based methods WaterNet and UWGAN.
Index Terms —CNN, Underwater image enhancement, Channel
attention, Color space
I. I NTRODUCTION
With the widespread aggravation of land resource shortage,
population expansion, and environmental degradation, which
pose potential hazards to the ocean——the crucial core of the
life-support system and the development of human society.
Consequently, various coastal countries have aroused their
environmental awareness and accelerated the research, devel-
opment, and utilization of the ocean. When exploiting marine
resources, the vision system is deemed as one of the most
effective detection methods for underwater robots. However,
the complexity of imaging in water and the difference of
law of light propagation cause inevitable degradation to the
underwater images such as color cast, low light, and low
contrast. Thus, underwater image enhancement with the aim
of restoring visually pleasing and clear images has become a
key issue in marine missions.
Earlier underwater image enhancement methods are rule-
based . The non-physical methods, such as histogram equal-
ization [1], auto white balance and fusion these two methods
to enhance underwater images [2], directly adjust the dynamic
value range and enhance the contrast by changing the grayvalue of the pixels in either spatial or frequency domain, with-
out considering the physical procedure of underwater degrada-
tion. These methods could remove certain noise and enhance
edge-details, whereas their performance is circumscribed due
to the physical characteristics of underwater imaging are not
taken into account.
Physical methods resolve such problems by abstracting and
modelling the underwater degradation process of imaging.
As such, undegraded images are accessible through inverting
the degradation process. For instance, B.L. McGlamery [3]
proposed the calculation model of underwater imaging system;
Jules S. Jaffe [4] optimized an underwater imaging system;
based on the imaging model, Nicholas c [5] proposed a
method through eliminating light scattering in underwater
images; He [6] proposed a single-image defogging method
based on dark channel prior (DCP). Generally, these physical
methods gain more superiority than the non-physical ones.
However, they are incapable of removing noise and blurring
artifacts from the underwater images, causing restrictions on
the quality of the processed images. To address this issue,
researchers introduced restoration techniques to obtain more
realistic underwater images. Lu [7] proposed the color cor-
rection method based on spectral characteristics to restore
distorted colors; Peng [8] proposed a depth estimation method
based on blur and absorption (IBLA); Song [9] proposed a
depth estimation method combined underwater light attenua-
tion prior with linear regression model to estimate background
light and transmitted light in RGB color space. Nevertheless,
these methods of image enhancement are usually based on the
premise of obtaining prior knowledge and information, thus
can only be managed under speciﬁc scenarios.
Beneﬁting from the development of neural networks,
learning-based methods gain extensive implementation in
image defogging, deblurring, and restoration, etc. Research
demonstrates that Convolutional Neural Network (CNN) could
exert better performance than the traditional methods both
objectively and subjectively. Inspired by the previous work,
researchers introduced CNN to improve the quality enhance-
ment of underwater images. Chen [10] proposed a scheme
combined a ﬁltering-based with a GAN-based restoration.
Li [11] designed an end-to-end WaterGAN consisting of
a depth estimation module followed by a color correction
module. Wang [12] proposed UWGAN learning the nonlinear
mapping between undistorted and distorted images. All abovemethods generate enhanced results by utilizing end-to-end
and data-driven training mechanisms and obtaining better
performance than rule-based methods. On the other hand,
these CNN models are computationally expensive because
they generally cost millions of parameters, which challenges
practical applications.
Instead, this paper systematically proposes an efﬁcient,
low complexity framework for underwater image enhance-
ment. Our framework consists of two stages, CNN-based
enhancement and YUV-based post-processing. Speciﬁcally, we
develop a simple yet effective network structure to extract
the latent features of input low-quality underwater images.
The network cascades three residual groups for feature en-
hancement and channel attention block is incorporated into
each residual group. Afterwards, the output of the CNN
models is transformed into YUV color space for luminance
correction. Besides, we design a collaborative loss function
containing perceive loss, MSE loss, gradient loss, and SSIM
loss to preserve the texture features. Results show that our
proposed framework outperforms state-of-the-art methods both
quantitatively and qualitatively.
II. R ELATED WORK
A. Learning-based Underwater Image Enhancement
As an effective deep learning method, CNN has been widely
adopted in the ﬁeld of visual enhancement. To continuously
promote, scientists also try to apply this method to the en-
hancement of underwater images.
Due to the arduousness of obtaining high-quality underwa-
ter reference images, Cameron. [13] proposed to use Cycle
Generative Adversarial Network (CycleGAN) [14] to improve
the quality of underwater visual scenes. Similarly, introduc-
ing gradient descent Loss to enhance the predictive ability
of the generated network; Wang [12] proposed an unsu-
pervised generative opposition network (UWGAN), which
utilized aerial image, depth map pairs, and optimized imaging
models to generate realistic underwater images. Speciﬁcally,
the obtained comprehensive underwater data further used the
U-Net network to directly reconstruct the underwater clear
image, while maintaining the structural similarity of the scene
content; Li [15] constructed an Underwater Image Enhance-
ment Benchmark (UIEB) including 950 real world underwater
images, 890 of which have the corresponding reference im-
ages, and propose an underwater image enhancement network
(called Water-Net); Zhang [16] proposed an attention-based
neural network to generate high-quality enhanced low-light
images from raw sensor data; Li [17] proposed an under-
water image enhancement with image colorfulness measure.
This model encompasses a pre-processed non-parametric layer
and an adaptively reﬁned parameter layer, optimizing the
enhancement network based on the joint of pixel level and
feature level; Li [18] proposed a lightweight convolutional
enhancement network (UWCNN) with good applicability. The
method is based on the synthetic underwater image training
set to directly reconstruct clear underwater images, which hasa good real-time characteristic and can be easily extended to
the frame-by-frame enhancement of the video.
Compared with the conventional methods, the CNN-based
methods above could achieve notable performance. Neverthe-
less, their computational complexity is greatly increased owing
to the larger cost of parameter and longer runtime than the
rule-based methods. Some networks like UWCNN has fewer
parameters than other methods like UWGAN and WaterNet,
whereas its performance is also compromised.
B. Underwater Image Dataset
Deep learning is a data-driven learning method. FUniE-
GAN opens source data set EUVP which is divided into
three subsets. These real underwater images from seven dif-
ferent camera equipment are cropped part of images from the
Youtube videos, and the ground truth is generated by a trained
CycleGAN [19]. UFO120 [20] is a new dataset divided into
training dataset and test benchmark dataset. It can be used
for model training tasks for underwater image super-division.
Li [15] provided paired dataset called UIEB where the original
image comes from the real underwater image, and the ground
truth is selected from a variety of traditional methods by
comparing. The dataset includes 890 training pictures and 90
test pictures, and small pictures are very convenient to test
and are commonly used. The author further used this dataset
to design WaterNet for underwater image enhancement. In this
paper, we perform our experiments using UIEB dataset.
III. P ROPOSED METHOD
In this section, we will describe the framework of our pro-
posed underwater image enhancement methods in details, and
introduce the loss function employed in our work. Ultimately,
we will present the post-processing employed, i.e., luminance
correction in YUV color space.
A. Network Architecture
Overall Framework. As illustrated in Fig. 1, the method
proposed consists of a single branch network and a post-
processing operation. The input image is an unprocessed RGB
underwater image of size 256 2563. Our network ﬁrst uses
3332 and 3364 convolution kernels to perform ﬂat
convolution operations to extract shallow features, and then
three residual groups are used to extract deeper features.
In each residual group, three channel attention blocks are
utilized to learn the distribution weights of different channels
to strengthen useful features and suppress invalid features. In
the CA block module, input data is squeezed into size of
11C data by average pooling, then this 1 1C descriptors
are put through the convolutional layers and activated by
sigmoid function. The output of the third residual group is
25625616. After processing by the convolution kernel with
a size of 33, an enhanced image is generated with a size of
2562563.
Channel attention module With notable breakthroughs in
both image and natural language processing, the AttentionFig. 1: Structure of our proposed network which concatenates three residual groups. Each residual group has three channel
attention blocks for latent feature extraction and texture preservation.
mechanism has been substantiated in improving network per-
formance. Drawing on the human brain and the human eye
perception mechanism, the attention mechanism intelligently
focuses attention on the local information of interest and
neglects extraneous information as much as CNN could. The
mechanism was proposed in 2018 by Jie Hu [21] and
extensively utilised in deep learning scenarios.
Fig. 2: Channel attention module used in our residual group.
As demonstrated in Fig. 2, H,W, and Care height, width,
and the number of channels, respectively. ris the reduction
factor. At ﬁrst, the input of size H WC features is processed
to generate channel weight of 1 1 for each channel through
pooling operation. Next, through using the convolutional and
sigmoid layers, the network learns the degree of dependence
across channels to obtain the weight of each channel. Finally,
the feature map of each channel with the corresponding
weighted value will be multiplied together to generate the
adjusted feature map of the same size as the input.
B. Loss Function
To ensure the generation of high-quality visual images, we
adopt a collaborative loss function in our work: content percep-
tion loss, mean square error(MSE) loss, structural similarity
(SSIM) loss, and gradient descent loss. The ﬁnal loss function
is presented as:
L=wvggLvgg+wmseLmse
+wssimLssim+wgdlLgdl;(1)wherewindicates the weighting factor.
Lvggis a content loss item which drives the network to
get training results with similar content. The ReLU activation
layer based on the pre-trained 19-layer VGG network deﬁnes
Lvggas:
Lvgg(E;G) =1
CjHjWjNX
i=1jj'j(Ei) 'j(Gi)jj;(2)
whereEis the enhanced image and Gis the reference image.
Ndenotes batch size, and Cj,Hj,Wjindicate the number,
height, and width of feature maps, respectively.
Lmse is the MSE loss between the enhanced image and
the reference image. Let nandmrepresent the width and
height of the image, respectively. We use Lmse to calculate
the average pixel-wise distance on the enhanced image and
the ground truth. It is written as:
Lmse=1
mnm 1X
i=0n 1X
j=0jjE(i;j) G(i;j)jj2: (3)
Lssim denotes the SSIM loss between enhanced image and
reference image:
SSIM (E;G) =(2EG+c1)(EG+c2)
(2
E+2
G+c1)(2
E+2
G+c2)(4)
Lssim(E;G) = 1 SSIM (E;G) (5)
whereE,Grepresents the average of image EandG,
respectively, E,Gmeans standard deviation of E and G,
EGmeans the covariance of E and G, c1andc2are constant
to avoid situations where the denominator is 0.
Lgdlis the gradient descent loss which introduces enhanced
image, reference image, and correlation between adjacent
pixels.means an integer greater than or equal to 1, in ourexperiment, we set to 1. The relevant formula is expressed
as follows:
Lgdl=X
i;jjjGi;j Gi 1;jj jEi;j Ei 1;jjj
+X
i;jjjGi;j Gi;j 1j jEi;j Ei;j 1jj:(6)
C. Post-processing in YUV Color Space
Our proposed method adds a post-processing module to
improve the overall brightness of the image generated by
the network. To elaborate further, we convert the original
image from RGB space to YUV space. YUV includes three
components Y , U, and V . Y stands for luma, U and V stand
for chroma respectively. Then we perform maximum and
minimum normalization operation on the Y component and
convert the original data linearization method to the range of
[0, 1]. The corresponding formula is as follows, MAX and
MIN in the formula respectively represent the maximum and
minimum of the Y channel component:
Y0= 255:0Y MIN
MAX MIN: (7)
IV. E XPERIMENTAL RESULTS
A. Implementation Details
The dataset of our project is mainly based on UIEB’s
890 high-deﬁnition underwater images with corresponding
reference images. Speciﬁcally, 800 are randomly selected as
the training set, while the remaining 90 images along with
10 images randomly selected from the UFO120 dataset [20]
constitute the test set. We resized the original picture and
cropped them to size 256 256 for better training and test. We
train the model use Adam, set the segmented learning rate from
0.001, and set the batch size to 16 and epoch to 500. We use
Tensorﬂow as the deep learning framework on an Inter(R)i7-
9700k CPU@3.60GHz, 32GB RAM, and a NVIDIA GeForce
RTX 2080Ti GPU. The total training time is around 10 hours.
B. Evaluation Metrics
We employ Peak Signal-to-Noise Ratio (PSNR) and Struc-
tural Similarity (SSIM) to measure the image quality, and use
Underwater Image Quality Measure (UIQM) [22] to evaluate
non-reference underwater quality assessments. The deﬁnition
of PSNR can be expressed as:
PSNR = 10log10(Max2
i
MSE); (8)
whereMax2
imeans the max pixel value of image provided
ranging from 0 to 255 for uint8 data type. For any picture,
PSNR reﬂects the degree of distortion of the image, and SSIM
objectively reﬂects how much the image is structural similarity
on scale. For the particularity of underwater image quality
assessment, the predecessors proposed UIQM to evaluate the
overall underwater image quality, which can intuitively reﬂect
image color, sharpness and contrast. UIQM is a reference-freeTABLE I: Performance comparison with state-of-the-art meth-
ods
Method SSIM PSNR (dB) UIQM
IBLA [8] 0.6672 15.5341 1.7402
DCP [6] 0.6657 13.5033 1.9462
UDCP [23] 0.5349 12.0763 1.7544
HE [24] 0.7453 15.6528 2.6197
CLAHE [25] 0.8218 18.2440 2.8704
ULAP [9] 0.6870 15.1370 1.7686
UWGAN [12] 0.7937 17.0210 3.1170
UWCNN [26] 0.7179 15.5202 2.8419
WaterNet [15] 0.8000 18.7366 2.7165
Ours 0.8720 21.9273 3.0522
index based on human visual system excitation. Aimed at the
degradation of underwater images, the UIQM is expressed as
a linear combination of three parts: Color Measurement index
(UICM), Degree Measurement Index (UISM), and Contrast
Measurement Index (UIConM). The calculation of UIQM is
as follows:
UIQM =c1UICM +c2UISM +c3UIconM; (9)
wherec1,c2, andc3indicate the weighting factors.
UICM = 0:0268q
2
;RG +2
;YB
+ 0:1586q
2
;RG+2
;YB(10)
UISM = 3
c=1cEME (grayscaleedge c) (11)
EME =2
k1k2k1
l=1k2
k=1log(Imax;k;l
Imin;k;l) (12)
UIConM = logAMEE (Intensity )
=1
k1k2O
k1
l=1k2
k=1(Imax;k;l Imin;k;l
Imax;k;lLImin;k;l)(13)
In UICM calculation, ;RG , and;YB mean asymmetri-
cal trimmed chroma intensity average in RG and YB. 2
;RG
and2
;RG mean the pixel variance in RG and YB.
In UISM calculation, grayscale edge representative gray
edge map of each R,G,B,EME is a method of measuring
edge sharpness and the values of the linear combination
areR=0.299,G=0.587,B=0.114. In EME, k1,k2mean
number of image divisions andImax;k;l
Imin;j;lmeans the relative
contrast.
In UIConM, k1,k2mean number of image divisions andN,Landmean the special operations on images.
C. quantitative comparison
We compare our work with ten state-of-the-art methods, as
shown in Table. I. Findings from this project indicate that our
proposed method achieves the highest SSIM and PSNR of all,
signiﬁcantly improves the SSIM, PSNR, and UIQM metrics,
e.g. For instance, the average SSIM, PSNR, and UIQM of
recent WaterNet is 0.8000, 18.7366dB, 2.7165, whereas ours
is as high as 0.8720, 21.9273, and 3.0522.(a) Underwater scenes with creatures
(b) Underwater scenes with yellowish tone
(c) Underwater scenes with bluenish tone
(d) Underwater scenes with greenish tone
(e) Low-light illumination underwater scenes
Fig. 3: Visualization results of different methods in low-light illumination underwater scenes.(a) Input Raw underwater images
(b) Underwater images enhanced without post-processing
(c) Underwater images enhanced with post-processing
(d) Ground truth underwater images
Fig. 4: Visual quality comparison of using and without using
post-processing in our approach.
D. Qualitative Comparison
The enhanced images of different methods are visualized in
Fig. 3 for comparison. In the ﬁgure, the last column provides
the ground truth images for reference. There are several diverse
underwater scenes such as (a) Scenes with creatures, (b)
Yellowish scenes. (c) Blueish scenes. (d) Greenish scenes.
(e) low-light illumination scenes. UW-GAN, UWCNN, and
WaterNet are three state-of-the-art CNN-based methods. It is
shown in the ﬁgure that UW-GAN fails in blurring removal,
color cast correcting, and chroma and brightness restoring,
e.g., in Fig. 3d, the visualization of UW-GAN is unclear
and unnatural. UWCNN is a simple network with a small
number of parameters. The images enhanced by UWCNN have
distinct color cast (Fig. 4c and Fig. 4d) and blurry details
(Fig. 3d). Among all the other methods, WaterNet and ours
conduct more efﬁcient in visual quality, yet WaterNet performs
worse than ours, especially in scenes like Yellowish and low-
light scenes. By contrast, the underwater images enhanced by
our method are much closer to the ground truth and visually
pleasing. Ultimately, the qualitative comparison proves again
the superiority of our proposed method.
E. Ablation Study
In order to verify the contribution of each stage in our
proposed framework, we conduct ablative experiments on theloss function and post-processing respectively.
1) Loss function: To verify the effectiveness of the percep-
tual loss, gradient loss, structural similarity loss, and mean
square error loss used in our method, we conduct a series of
ablation experiments on the four loss functions respectively.
Each time a certain loss function item is disabled. The exper-
imental results are shown in Table II.
TABLE II: Loss function ablation experiment
Method SSIM PSNR (dB) UIQM
loss in ours 0.8720 21.9273 3.0522
w/oLvggloss 0.8544 20.2642 3.0068
w/oLmse loss 0.8242 19.8349 3.1453
w/oLssim loss 0.8151 19.8073 3.0888
w/oLgdlloss 0.8651 21.3996 3.0661
Comparing the data Table II, we ﬁnd that the loss proposed
can obtain the highest SSIM and PSNR. Without using Lgdl,
the UIQM value is slightly improved at the expense of
decreasing UIQM and SSIM. Without using Lmse, nothing
could be higher with a dramatic decline except UIQM. In
general, our proposed collaborative loss successfully reach a
balance across SSIM, PSNR, and UIQM, leading to high-
quality underwater images.
2) Post-processing: In order to verify the contribution of
the post-processing operation, we conduct a post-processing
ablation experiment in which the post-processing is removed.
We can see from Table. III that without the post-processing
operation, both SSIM and PSNR are greatly decreased al-
though there is a slight decline in UIQM. We also visualize the
results in Fig. 4. It is obvious that using the post-processing
signiﬁcantly improves the the sharpness and brightness of the
images.
TABLE III: Efﬁciency comparison experiment results
Method SSIM PSNR (dB) UIQM
w/o post-processing 0.8606 21.3234 3.0963
Ours 0.8720 21.9273 3.0522
F . Computational Complexity
In addition to the quantitative and qualitative comparisons
above, we analyze the number of parameters and the runtime
of these CNN-based methods for comparison, as shown in Ta-
ble IV. Although UWCNN has fewer parameters and UWGAN
has less runtime than ours, their performance is much lower
than ours. The number of parameters in WaterNet is 1106k,
which is 2.5 times our number. The runtime of WaterNet is
also 3 times that of ours. Instead, our method reaches the
highest performance of all with low complexity cost.
V. C ONCLUSION
This paper proposes an efﬁcient, low-complexity framework
for underwater image quality enhancement. Our frameworkTABLE IV: computational complexity comparison with exist-
ing CNN-based methods
Method Parameter Runtime
UWCNN [18] 97901 0.2894
UWGAN [12] 1939194 0.0170
WaterNet [15] 1106274 0.6096
Ours 436682 0.2025
consists of two stages: CNN-based enhancement and post-
processing operation. In terms of the CNN-based enhance-
ment, we design a CNN structure using cascading residual
groups for feature extraction and enhancement. In each resid-
ual group, the channel attention mechanism is incorporated
to self-adjust the channel weight coefﬁcients of each channel.
We observe that the images generated from CNN are of low
contrast. To this end, we feed the images to the post-processing
stage for further brightness improvement. Speciﬁcally, the
image is transformed into YUV space and the Y component is
normalized and corrected. We conduct our experiments on the
conventional UIEB dataset. Results demonstrate that compared
with state-of-the-art methods, our proposed approach achieves
the highest SSIM and PSNR values of all. Besides, we also
compare the visual quality of all methods. Results conﬁrm that
the underwater images enhanced by our approach look much
better than those of other methods and much closer to the
ground truth. In terms of the computational complexity, our
method costs only 436k parameters and our runtime is only
1=3of existing method WaterNet.
REFERENCES
[1] C.-Y . Li, J.-C. Guo, R.-M. Cong, Y .-W. Pang, and B. Wang, “Underwater
image enhancement by dehazing with minimum information loss and
histogram distribution prior,” IEEE Transactions on Image Processing ,
vol. 25, no. 12, pp. 5664–5677, 2016.
[2] C. Ancuti, C. O. Ancuti, T. Haber, and P. Bekaert, “Enhancing underwa-
ter images and videos by fusion,” in 2012 IEEE conference on computer
vision and pattern recognition . IEEE, 2012, pp. 81–88.
[3] B. McGlamery, “A computer model for underwater camera systems,”
inOcean Optics VI , vol. 208. International Society for Optics and
Photonics, 1980, pp. 221–231.
[4] J. S. Jaffe, “Computer modeling and the design of optimal underwater
imaging systems,” IEEE Journal of Oceanic Engineering , vol. 15, no. 2,
pp. 101–111, 1990.
[5] N. Carlevaris-Bianco, A. Mohan, and R. M. Eustice, “Initial results in
underwater single image dehazing,” in Oceans 2010 Mts/IEEE Seattle .
IEEE, 2010, pp. 1–8.
[6] K. He, J. Sun, and X. Tang, “Single image haze removal using dark
channel prior,” IEEE transactions on pattern analysis and machine
intelligence , vol. 33, no. 12, pp. 2341–2353, 2010.
[7] H. Lu, Y . Li, L. Zhang, and S. Serikawa, “Contrast enhancement for
images in turbid water,” JOSA A , vol. 32, no. 5, pp. 886–893, 2015.[8] Y .-T. Peng and P. C. Cosman, “Underwater image restoration based
on image blurriness and light absorption,” IEEE transactions on image
processing , vol. 26, no. 4, pp. 1579–1594, 2017.
[9] W. Song, Y . Wang, D. Huang, and D. Tjondronegoro, “A rapid scene
depth estimation model based on underwater light attenuation prior for
underwater image restoration,” in Paciﬁc Rim Conference on Multimedia .
Springer, 2018, pp. 678–688.
[10] X. Chen, J. Yu, S. Kong, Z. Wu, X. Fang, and L. Wen, “Towards quality
advancement of underwater machine vision with generative adversarial
networks,” 2018.
[11] J. Li, K. A. Skinner, R. M. Eustice, and M. Johnson-Roberson, “Water-
gan: Unsupervised generative network to enable real-time color correc-
tion of monocular underwater images,” IEEE Robotics and Automation
letters , vol. 3, no. 1, pp. 387–394, 2017.
[12] N. Wang, Y . Zhou, F. Han, H. Zhu, and J. Yao, “Uwgan: underwater
gan for real-world underwater color restoration and dehazing,” arXiv
preprint arXiv:1912.10269 , 2019.
[13] C. Fabbri, M. J. Islam, and J. Sattar, “Enhancing underwater imagery
using generative adversarial networks,” in 2018 IEEE International
Conference on Robotics and Automation (ICRA) . IEEE, 2018, pp.
7159–7165.
[14] J.-Y . Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image
translation using cycle-consistent adversarial networks,” in Proceedings
of the IEEE international conference on computer vision , 2017, pp.
2223–2232.
[15] C. Li, C. Guo, W. Ren, R. Cong, J. Hou, S. Kwong, and D. Tao, “An
underwater image enhancement benchmark dataset and beyond,” IEEE
Transactions on Image Processing , vol. 29, pp. 4376–4389, 2019.
[16] C. Zhang, Q. Yan, Y . Zhu, X. Li, J. Sun, and Y . Zhang, “Attention-based
network for low-light image enhancement,” in 2020 IEEE International
Conference on Multimedia and Expo (ICME) . IEEE, 2020, pp. 1–6.
[17] H. Li, X. Yang, Z. Li, and T. Zhang, “Underwater image enhancement
with image colorfulness measure,” arXiv preprint arXiv:2004.08609 ,
2020.
[18] C. Li, S. Anwar, and F. Porikli, “Underwater scene prior inspired deep
underwater image and video enhancement,” Pattern Recognition , vol. 98,
p. 107038, 2020.
[19] Q. You, C. Wan, J. Sun, J. Shen, H. Ye, and Q. Yu, “Fundus image
enhancement method based on cyclegan,” in 2019 41st annual inter-
national conference of the IEEE engineering in medicine and biology
society (EMBC) . IEEE, 2019, pp. 4500–4503.
[20] M. J. Islam, P. Luo, and J. Sattar, “Simultaneous enhancement and super-
resolution of underwater imagery for improved visual perception,” arXiv
preprint arXiv:2002.01155 , 2020.
[21] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in
Proceedings of the IEEE conference on computer vision and pattern
recognition , 2018, pp. 7132–7141.
[22] K. Panetta, C. Gao, and S. Agaian, “Human-visual-system-inspired
underwater image quality measures,” IEEE Journal of Oceanic Engi-
neering , vol. 41, no. 3, pp. 541–551, 2015.
[23] P. L. Drews, E. R. Nascimento, S. S. Botelho, and M. F. M. Campos,
“Underwater depth estimation and image restoration based on single
images,” IEEE computer graphics and applications , vol. 36, no. 2, pp.
24–35, 2016.
[24] R. Hummel, “Image enhancement by histogram transformation,” Un-
known , 1975.
[25] S. M. Pizer, “Contrast-limited adaptive histogram equalization: Speed
and effectiveness stephen m. pizer, r. eugene johnston, james p. ericksen,
bonnie c. yankaskas, keith e. muller medical image display research
group,” in Proceedings of the First Conference on Visualization in
Biomedical Computing, Atlanta, Georgia , vol. 337, 1990.
[26] Y . Guo, H. Li, and P. Zhuang, “Underwater image enhancement using
a multiscale dense generative adversarial network,” IEEE Journal of
Oceanic Engineering , vol. 45, no. 3, pp. 862–870, 2019."
留宿佐证材料.docx,D:/tool/Pycharm/summerProject/examples\留宿佐证材料.docx,"留宿佐证材料
（参加学科竞赛、科研项目、学校组织的在杭社会实践学生用）

兹有杭州师范大学信息科学与技术学院           等同学（完整名单见附件），暑假因                    ，确需留宿在校，留宿时间为     月    日至    月    日，情况属实。
    特此证明。


指导教师（签名）：                 
    联系方式：                 



"
